# 前言

- [ ] 分离语言的数据结构实现
- [ ] 在每一个知识点上记录一下，思考一下时间和空间复杂度是如何计算的
- [ ] 找到一些比较好的参考资料

参考：[学习算法和刷题的思路指南](https://github.com/labuladong/fucking-algorithm/blob/master/%E7%AE%97%E6%B3%95%E6%80%9D%E7%BB%B4%E7%B3%BB%E5%88%97/%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%E7%9A%84%E9%AB%98%E6%95%88%E6%96%B9%E6%B3%95.md)

python的排序算法 属于高级实践算法

# 数据结构与算法
基础的东西还是会忘，目标是总结常用数据结构的定义，以及python底层实现

## 一、底层结构
### 1.1 数组(物理结构)
按照目前的定义，数组是一整块连续的内存地址，特点是**数据连续，支持快速随机访问**

>Q：为什么支持随机访问？为什么访问快速？
>A：随机访问是指访问一个中间元素时不需要经过前面的元素；访问快速的原因是知道头元素的位置，根据内存位置计算就可以得到目标的位置，时间复杂度O(1)
>
>吐槽：我随机出一个数字n，然后找到链表中的第n个元素，这不就是随机访问吗？在我来看，随机的意思是指找到任意一个，但并没有指定寻找的方式是直接访问还是通过遍历的间接访问；很多网上的解释说因为链表需要遍历，那这样的解释的意思不是说链表不能直接访问吗？随机和跟直接是一个意思吗！？这句话“数据连续，支持快速随机访问”如果改成“数据连续，支持任意位置直接访问”不是更准确吗？说到底这里的随机本身有概率选择的含义在，而想表达的东西又跟概率没有任何关系，只不过想说可以直接访问而已。

### 1.2 链表(逻辑结构)
链表的物理结构与数组不同，由**离散的内存地址构建的存储结构**

## 二、上层结构

### 2.1 线性表
**定义：**

1. 只有一个开始元素
2. 只有一个结束元素
3. 除开始和结束外，每个元素都只有一个前驱和一个后继
> 吐槽：我第一次学习的时候听到“一个前驱和一个后继”，总是自然而然地想到链表，因此对我来说用数组实现线性表总觉得怪怪的，现在才觉得所谓“一个前驱和一个后继”的意思其实不过是指线性表中的元素是有序的而已，所以你可以通过任意一个元素找到它的前一个或者后一个元素。
>
> 吐槽：另外个人觉得“实现线程表”这个说法其实也很奇怪，因为当我们在讨论线性表的时候实际上就是在讨论数组和链表本身，所以与其说用数组和链表实现线性表，不如说在讨论线性表中的数组和链表，因为数组和链表就是线性表这个概念下的两个类型。

**类型：**
- 数组：申请一块指定大小的内存
- 链表：无需提前申请

**链表类型：**

- 单链表：节点只往后指
- 双向链表：节点往前往后都可以指
- 循环链表：尾节点指向头节点

**比较：**

|时间复杂度|查找|删除|插入|
|----|----|----|----|
|数组|O(1)|O(n)|O(n)|
|链表|O(n)|O(n)|O(1)|

> 吐槽：这里有个很有意思的点：链表的删除操作的时间复杂度是多少？如果仅仅考虑删除操作，那么时间复杂度应该是O(1)，但如果考虑删除前需要定位被删除的元素那么就需要先调用查找操作，所以时间复杂度变为$O(n) + O(1) = O(n) $

### 2.2 栈(stack)
**定义：**

1. 表的末端称为**栈顶**
2. 表只允许**进栈**和**出栈**两种操作，也叫做**后进先出表(LIFO)**

**应用：**

1. 代码检查中的括号匹配
2. 数学表达式计算(<font color=red>todo</font>)
3. 函数栈帧(<font color=red>todo</font>)

**时间复杂度：**
均为O(1)

**code**

```python
def check_brackets(s: str):
    stack = list()
    bracket_map = {
        '[': ']',
        '{': '}',
        '(': ')',
    }
    bracket_types = list(bracket_map.keys()) + list(bracket_map.values())
    for i in s:
        if i not in bracket_types:
            continue
        if not stack:
            stack.append(i)
        else:
            b = stack.pop()
            if bracket_map[b] != i:
                return False
    if stack:
        return False
    else:
        return True
```

### 2.3 队列(queue)
**定义：**
1. 表的末端叫做**队尾**，首端叫做**队首**
2. 队列只能从队尾插入，队首删除，即**入队(enqueue)**和(**出队(dequeue)**，队列也叫做**先进先出表(FIFO)**

**应用：**
1. 通信，例如redis的list

**时间复杂度：**
均为O(1)


### 2.4 树(tree)
参考：

- 树相关的属性和性质：https://www.cnblogs.com/guoyaohua/p/8595289.html
- 二叉树的性质：https://www.jianshu.com/p/20c8881c6b86
- 各种树的定义：https://blog.csdn.net/u014532217/article/details/79118023
- 二叉搜索树(BST)与平衡二叉树(AVL树)专题：https://blog.csdn.net/DaveBobo/article/details/77603549
- 构建二叉树：https://developer.aliyun.com/article/616378
- 构建二叉树python实现：https://www.pythonf.cn/read/134131
- 二叉树的删除操作：https://blog.csdn.net/zxnsirius/article/details/52131433
- 平衡二叉树旋转示意图：https://www.jianshu.com/p/d802766551ff
- 平衡二叉树python实现：https://blog.csdn.net/qq_34840129/article/details/80728186
- 二叉树遍历递归与迭代实现：https://zhuanlan.zhihu.com/p/56895993

**定义：**
树是多个节点的集合，若非空，存在一个**根结点(root)**，根结点下有一个或多个子树，通过边连接

**概念**
- 结点的上方结点称为**父结点**
- 结点的下方结点称为**子结点**
- 子结点之间互为**兄弟节点**
- 树的最下方节点称为**叶子结点**
- 每个结点拥有的子结点数称为**度**

**类型：**

- 无序树：树的任意节点的子节点没有顺序关系。
- 有序树：树的任意节点的子节点有顺序关系。
- 二叉树
	- 满二叉树：叶子节点都在同一层并且除叶子节点外的所有节点都有两个子节点。
	- 完全二叉树：叶子结点在最后一层或者倒数第二层，完全二叉树最下面一层的叶子结点未满
	- 二叉查找树（二叉搜索树、BST）
	- 平衡二叉树（AVL）
- 霍夫曼树
- 红黑树
- B树/B+树/B\*树

**实现：**
树由类似链表中的结点连接形成，但跟链表不同的是，树中结点的连接顺序不固定，相较于线性表多了一个维度，因此添加、删除等操作不一定会产生唯一的结果，所以在此只讨论单个结点如何实现

一般单个结点存放**自身的值**、**第一个子结点**，以及**下一个兄弟结点**，然后延伸下去就可以存放整棵树的信息

```c
# PrToNode代表指向结构体TreeNode的指针
typedef struct TreeNode *PtrToNode;

struct TreeNode{
    ElementType Element;
    PtrToNode 	FirstChild;  # 第一个子结点
    PtrToNode 	NextSibling; # 下一个兄弟结点
}
```

**应用**
1. 文件目录系统

#### 2.4.0 二叉树

**定义：**

1. 树的任意节点至多包含两棵子树。

**性质：**
1. 在非空二叉树中，第n层的结点总数不超过$2^{n-1}$
2. 深度为n的二叉树最多有$2^{n}-1$个结点，最少有$n$个结点

**推导性质**

1. 对于任意一棵二叉树，如果其叶结点数为$N_0$，而度数为2的结点总数为$N_2$，则$N_0=N_2+1$，原因是$2^{i-1} = 2^{i-1} -1 + 1$
2. 具有$n$个结点的完全二叉树的深度为$h =[log_2n]+1$(向下取整)，原因是$[log_2n] = [log_2(2^h-1)] = n - 1$

**实现：**

```c
typedef struct TreeNode *PtrToNode;
typedef struct PtrToNode Tree;

struct TreeNode{
    ElementType Element;
    Tree				Left;
    Tree				Right;
}
```

**code**
<font color=red>二叉树添加算法</font>


#### 2.4.1 二叉搜索树(BST)
**定义：**

1. 若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值
2. 若任意节点的右子树不空，则右子树上所有节点的值均大于它的根节点的值
3. 任意节点的左、右子树也分别为二叉查找树
4. 没有键值相等的节点

**时间复杂度：**
索引时间复杂度O(h)，h为树的层数，因为同样的数据可能组合出来的二叉搜索树有很多情况，而时间复杂度是由**树的层数**决定的

#### 2.4.2 平衡二叉树(AVL)
**定义：**

1. 它是一棵空树或它的左右两个子树的高度差的绝对值不超过1
2. 并且左右两个子树都是一棵平衡二叉树
3. 平衡二叉树必定是二叉搜索树

**时间复杂度：**
对于有n个结点的二叉平衡树，O($log_2(n)$)

#### 2.4.3 霍夫曼树
**定义：**

1. 带权路径最短的二叉树称为哈夫曼树或最优二叉树

#### 2.4.4 红黑树

#### 2.4.5 B树/B+树

#### 2.4.4、二叉树创建和添加结点

[二叉树实现代码](code/binary_tree.py)

#### 2.4.5、二叉搜索树
这些迁移到实现算法中

[BST树具体实现代码](code/bst.py)

二叉搜索树的生成大概就是修改了普通二叉树的添加元素操作，改为了插入，每次添加节点时都会比较结点对应值的大小，然后在合适的位置插入；那二叉平衡树又是如何添加结点的呢？

通过参考文章中的[二叉搜索树](https://blog.csdn.net/zxnsirius/article/details/52131433)这篇，我大概懂了之前我上面看到的python实现二叉树那篇文章中的删除逻辑，正如之前说的，被删除的结点同时有左子树和右子树的时候会比较麻烦，但对于搜索二叉树来说，删除后还符合定义的一般就是左子树的最右结点和右子树的最左结点这两个。

通过上面的可执行代码，我发现一个问题，例如下面的树
```
tree_1:
                     10                                              
         5                       20                      
   0           *           15          25          
*     3     *     *     12    17    22    30  
tree_2:
                     10                                               
         3                       20                      
   0           *           15          25          
*     *     *     *     12    17    22    30 
tree_3:
                     10                                              
         0                       20                      
   *           3           15          25          
*     *     *     *     12    17    22    30 
```

试问tree_1中的5结点被删除后，是变成tree_2呢？还是变成tree_3呢？如果删除逻辑是把5的左子树直接给10的左子树的话，就是tree_3；如果像我一样，会去找左子树中的最大值，然后替换删除就是tree_2；

```
tree_1:
                     5
         4                       6
   2           *           *           8
1     3     *     *     *     *     7     9
tree_2:
                     4
         2                       6
   1           3           *           8
*     *     *     *     *     *     7     9
tree_3:
                     4
         3                       6
   2           *           *           8
1     *     *     *     *     *     7     9
```
同样，被删除的是10结点的话，也会有这样的问题；这两者是否有优劣之分呢？

> 两种删除都已经实现，但感觉逻辑还没有达到最精简的程度，依旧有些绕

#### 2.4.6、二叉平衡树

[AVL树具体实现代码](code/avl.py)

**概念**
bf值：当前结点的左子树高度减去右子树的高度之差
旋转算法：插入和删除操作后任然要保证是平衡的，是通过一个旋转算法实现的

记录bf值跟记录高度本质应该是一样的，一般会创建一个计算高度或者bf值的函数，或者，每次插入删除后都修改一遍属性
**关键**
1、avl树添加元素的关键我感觉只有两个，一个是旋转方向的判定，另一个是旋转算法的实现
- 方向判定
判断旋转方向需要三个结点：插入结点，不平衡结点左结点/右结点，以及不平衡结点
如果插入结点的值小于不平衡结点，那么就是第一个l，如果同时小于不平衡结点的左结点，那就是ll，如果大于不平衡结点的左结点，那就是lr；
如果插入结点的值大于不平衡结点，那么就是第一个r，如果同时小于不平衡结点的右结点，那就是rl，如果大于不平衡结点的右结点，那就是rr；
- 旋转算法逻辑
如果是ll，插入结点和不平衡结点变成不平衡结点左结点的两个子结点；
如果是rr，插入结点和不平衡结点变成不平衡结点右结点的两个子结点；
如果是lr，就会出现第三个关键结点：待拆分结点，待拆分结点的左右子树被拆分出来，左子树作为不平衡结点左结点的右结点，右子树作为不平衡结点的左结点，而不平衡结点的左结点和不平衡结点本身成为待拆分结点的左右结点；
如果是rl，待拆分结点的左右子树被拆分出来，左子树作为不平衡结点的右结点，右子树作为不平衡结点右结点的左结点，而不平衡结点和不平衡结点的右结点本身成为待拆分结点的左右结点；

2、avl的删除，与插入不同，当删除一个结点的时候，可能会出现多个不平衡的情况，例如下面删除结点8的时候，就会出现5-4-3这种ll型的，以及5-4-4.5这种lr类型的。

```
         5
   4           8
3   4.5     *     *
```
我能想到的粗暴方法就是，直接找出高度最高的一层结点，然后取第一个，处理方法将该结点等价于插入结点再平衡，如下。
```
         4
   3           5
*     *   4.5     *
```

**复杂度**
查询$O(logN)$，频繁旋转会使插入和删除牺牲掉$O(logN)$

**问题**
1、bf值每次要更新全部结点，感觉效率太低了，可以优化

#### 2.4.7、霍夫曼树
> 哈夫曼树又称最优二叉树，是一种带权路径长度最短的二叉树。所谓树的带权路径长度，就是树中所有的叶结点的权值乘上其到根结点的路径长度（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）。树的路径长度是从树根到每一结点的路径长度之和，记为WPL=（W1*L1+W2*L2+W3*L3+...+Wn*Ln），N个权值Wi（i=1,2,...n）构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为Li（i=1,2,...n）。可以证明哈夫曼树的WPL是最小的。

#### 2.4.7、B树，B+树，B\*树
B树和B+树非常典型的场景就是用于关系型数据库的索引(MySQL)

#### 2.4.8 红黑树


#### 2.4.E、二叉树的使用场景
比起二叉树的操作，我更加好奇这样的数据结构具体在什么样的场景中适合使用呢？在我看来，二叉树最大的优势就是查询速度快，但同样修改删除等操作很复杂抵销了部分优势。

> 哈夫曼编码，来源于哈夫曼树（给定n个权值作为n个叶子结点，构造一棵二叉树，若带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为赫夫曼树(Huffman tree)。即带权路径长度最短的树），在数据压缩上有重要应用，提高了传输的有效性，详见《信息论与编码》。
> 海量数据并发查询，二叉树复杂度是O(K+LgN)。二叉排序树就既有链表的好处，也有数组的好处， 在处理大批量的动态的数据是比较有用。
> C++ STL中的set/multiset、map，以及Linux虚拟内存的管理，都是通过红黑树去实现的。查找最大（最小）的k个数，红黑树，红黑树中查找/删除/插入，都只需要O(logk)。
> B-Tree，B+-Tree在文件系统中的目录应用。
> 路由器中的路由搜索引擎。


### 2.5、图
参考文章：
- [数据结构：图](https://www.jianshu.com/p/bce71b2bdbc8)
- [动画解析：图的遍历方式有哪些？](https://www.cxyxiaowu.com/9854.html)

我的实现：[图实现代码](code/graph.py)

> 在计算机科学中，一个图就是一些顶点的集合，这些顶点通过一系列边结对（连接）。顶点用圆圈表示，边就是这些圆圈之间的连线。顶点之间通过边连接。
> 边可以有权重（weight），即每一条边会被分配一个正数或者负数值。考虑一个代表航线的图。各个城市就是顶点，航线就是边。那么边的权重可以是飞行时间，或者机票价格。

一些定义：
- 有向图、无向图
- 完全图：假设有n个定点，有$\frac{1}{2}n(n-1)$条边，
- 稀疏图：边的数量小于nlogn
- 稠密图：边的数量大于nlogn
- 子图

实现方式：
- 邻接链表
- 邻接矩阵

遍历跟树类似
- 深度优先遍历
- 广度优先遍历

最短路径问题？

### 2.6、<font color=red>堆</font>

### 2.7、散列表(哈希表)
参考:
[Hash Tables and Hash Functions](https://www.youtube.com/watch?v=KyUTuwz_b7Q)
[数据结构与算法——散列表](https://zhuanlan.zhihu.com/p/70477750)

想象一批数据存在一个列表中，此刻我们有其中一个数据，然后想在列表中找到这个数据，要如何找呢？一种方法是线性查询，即从列表头部开始依次比较每个数据是否跟我们的数据相同，那有没有比较快速的方法找到数据呢？假设在一开始存放数据的时候，我们根据数据的值计算出一个位置，然后将数据存放到这个位置中，那么再次查找的时候只需要再次计算，直接取这个位置的数据就可以。

**定义**
什么是散列表？散列表是根据关键码值(Key value)而直接进行访问的数据结构`f(key) = address`，f就是哈希函数（散列函数）再加上处理冲突的方法构成了映射规则，那么通过这个映射规则将key映射到数据地址的表就叫做散列表。

普通的哈希算法

- 除留余数法
- 平方取中法

高级的哈希算法
- <font color=red>MD5</font>
- <font color=red>SHA</font>
- <font color=red>CRC </font>

**哈希冲突**
产生冲突的原因：哈希算法本质上是一种数据压缩算法，那么当数据量大到一定程度时就会不可避免出现不同的数据产生相同的哈希值这样的情况

**解决方法**
1、开放寻址法/再散列法
这种方法也称再散列法，其基本思想是：当关键字key的哈希地址p=H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。

- 线性探测再散列
- 二次探测再散列
- 伪随机数探测再散列

2、链地址法/拉链法
这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。

3、公共溢出区法
这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。

4、再哈希法
冲突后再使用一个不同的哈希函数

**python中的hash函数**
python是如何将不同类型的数据使用哈希函数计算出位置的呢？

## 三、算法
参考：
[10分钟详解：算法面试5大必考排序方式](https://zhuanlan.zhihu.com/p/74325524)

### 3.1 线性表的排序算法

评价排序算法的标准，除了时间和空间复杂度之外，还有一个就是**稳定**，也就是说如果数组里两个相等的数，那么排序前后这两个相等的数的相对位置保持不变，即为稳定，否则为不稳定。

||是否稳定|时间复杂度|空间复杂度|
|-----|-----|-----|------|
|冒泡排序|稳定|最好的情况$n-1$，最坏的情况$n*(n-1)/2$，所以平均是O($n^{2}$)|O(1)|
|插入排序|稳定|最好的情况$n-1$，最坏的情况$n*(n-1)/2$，所以平均是O($n^{2}$)|O(1)|
|归并排序|稳定|$nlog_2n$|O(n)|
|快速排序|不稳定|最优情况下复杂度是O($nlog_2n$)，最坏的情况是O($n^2$)|O($logn$)|
|希尔排序|不稳定|最坏的情况是O($n^2$)，最优和平均取决于所选的gap数列，以Sedgewick数列为例，最优是O($n^{1.3}$)|O(1)|
|选择排序|不稳定|O($n^2$)|O(1)|

todo 不同数据规模分别适合使用什么排序模式

冒泡排序、插入排序、希尔排序可以归为一类

|语言|代码实现|
|-----|-----|
|python|[python排序代码实现](../code/sort_list.py)|

#### 3.1.1 冒泡排序
**描述**
遍历线性表，每次比较两个相邻的元素，如果发现顺序不对，则对换位置；遍历完之后，从头开始，直到顺序ok

**优化**

1. 可以记录每次遍历中最后一个进行交换的位置，因为在这个位置之后的顺序都是正确的，下次遍历的时候，到这个位置就可以退出本次遍历
2. 其实不用每次都对换两个元素的位置，举例[5, 4, 3, 2]，我们如果用一个变量的值来保存5，每次将4， 3， 2赋给前面一个位置的值，然后将最后一个位置赋值为5就可以

#### 3.1.2 插入排序
**描述**
插入排序是基于插入操作，将第一个元素作为一个新表，遍历其他元素，判断其他元素在新表中的位置，然后执行插入操作

**优化**

在有序部分插入的时候可以用二分查找定位插入的位置

#### 3.1.3 归并排序
参考：[快速排序和归并排序的时间复杂度分析——通俗易懂](https://www.cnblogs.com/tuyang1129/p/12857821.html)

**描述**
归并排序是基于分治法的思想，每次将序列拆分两个子序列，每个子序列重复拆分成两个子序列，直到不能在拆分，然后每两个子序列在进行合并，然后返回一个有序的序列

**分析**
归并为什么更快一些？首先，它使用了一个新的数组，第二，它通过分治法使得每个子序列都是有序的，进而避免来插入操作，每次都是对一个新数组做添加操作

时间复杂度：首先会拆分多少层？$log_2n$层，然后每层合并都会遍历n个元素，所以时间复杂度是$nlog_2n$
空间复杂度：每次递归都会产生一个栈空间，这个大小固定，而每一层会产生一个栈空间，所以总的是O($log_2n$)，另外还需要一个新的数组，O($n$)
> 我第一次思考的时候觉得栈空间的数量肯定不止$log_2n$个，但后来我意识到每次cpu只会沿着一个函数往下执行，而不是立刻生成所有栈空间，所以这里确实是由深度决定的

**优化**

1. 判断两个子序列最小值和最大值是否有交集，没有的话可以直接合并
2. 对于数组有两个元素的子序列和只有一个元素的子序列，可以使用插入操作实现排序

#### 3.1.4 快速排序
**描述**
快速排序也是基于分治法，但与归并排序不同的时候，快排并不是随机将序列拆分成两个子序列，而是从序列中选出一个值作为**基准**，然后将小于这个值的元素移动到右边，大于这个值的元素移动到左边，这个操作叫做**分区**。详细步骤如下

1. 从数组中选出基准，保留基准的值`pivot`以及基准的索引坐标`pivot_index`
2. 接受两个游标，如`left`和`right`，一般开始默认为`left`为0，`right`等于数组最后一位的索引坐标，判断`left >= right`，如果为`True`直接退出；同时设置一个移位操作标志`flag`，默认为`false`，如果为`true`，说明出现了移位操作；这里也可以
3. 进入第一层while，退出条件是`left > right`，如果没有退出执行下面的逻辑
4. 进入第二层while，退出条件是`right > pivot_index or data[right] < pivot`，如果没有退出执行`right = right - 1`；如果退出这一层意味着，基准右边出现了一个小于基准的值或者基准右边已经没有小于基准的值了
4. 判断此时基准是否存在一个小于基准的值，判断条件是`if right > pivot_index`，如果为真，执行移位操作，`data[pivot_index] = data[right]; pivot_index = right`，以及修改移位操作标志，`flag = True`
5. 进入第三层while，退出条件是`left > pivot_index or data[left] > pivot`，如果没有退出执行`left =left + 1`；如果退出这一层意味着，基准左边出现了一个大于基准的值或者基准左边已经没有大于基准的值了
6. 类似步骤4，判断此时`left < pivot_index`是否成立，如果成立执行移位操作；第一层while的逻辑至此结束
7. 当退出第一层while后，判断`flag`是否为`True`，若为True，则执行`data[right] = pivot`，这里使用`right`或者`left`都可以，因为到最后`right`必定会等于`left`
8. 重复所有步骤，将`left`置为0，`right`置为`left-1`
9. 再次重复所有步骤，将`left`置为`right+1`，`right`置为`right`

**分析**
空间复杂度，和归并排序不同，快速排序在每次递归的过程中，只需要开辟O(1)的存储空间来完成交换操作实现直接对数组的修改，又因为递归次数为logn，所以它的整体空间复杂度完全取决于压堆栈的次数，因此它的空间复杂度是O(logn)。

稳定性：不稳定是因为交换的操作

快排相较于归并排序降低了空间复杂度，但是时间复杂度上如果出现最坏的情况，会比归并排序排序更差；但归并排序会频繁地创建和删除数组，所以对于大批量的数据，这些操作也会造成空间和时间效率的降低

**优化**

1. 对于快排来说，基准的选择会决定分区的次数，所以选择好的基准可以有效避免最差的情况，这里可以使用随机算法来随机选一个值，也可以很简单的，选择**最左、最右跟中间**三个值的中数作为基准
2. 三分区法，对于二分区（把序列分为大于和小于两个分区）来说，不太好处理跟基准刚好相等的值，所以可以将序列分为**大于、小于和等于**三个区
3. 小分区（元素数量小于10）使用插入排序


#### 4.5 希尔排序
参考：[Shell Sort Algorithm](https://www.programiz.com/dsa/shell-sort)
**描述**

>希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。

希尔排序就是按照步长从数组中拆分出一个小批量的分片，然后对这个分片做插入排序。具体步骤如下：
1. 首先根据数组的长度选择一个步长(`gap`)，一般默认为数组长度(n)的一半
2. 进入第一层`while`，当`gap > 0`时执行以下逻辑
3. 开始遍历`for index in range(gap, n)`，将`index`存为`current_index`
4. 进入第二层`while current_index >= gap and data[current_index] < data[current_index-gap]`，如果成立，则`data[current_index] = data[current_index-gap]`，然后`current_index = current_index - gap`
4. 退出第二层`while`后，执行`data[current_index] = data[index]`
5. 退出遍历之后，`gap = int(gap / 2)`
6. 退出第一层`while`

**分析**
希尔排序属于插入排序的优化，一般如果最小值在最后一位，那么需要移动和比较大次数就很多，而希尔排序通过增量跳跃的方式使得那些需要移动很远位置的元素可以跳跃式地快速移动到对应的位置，节省了很多操作。

时间复杂度：不同的gap数列会有不同的时间复杂度，选择比较复杂的数列，可以获得O($n^{1.5}$)的时间复杂度

**优化**

1. 一般的gap的数列可能会导致无用的操作

> 最好的增量序列是 Sedgewick提出的 (1, 5, 19, 41, 109,...)，该序列的项来自 $9 * 4^i - 9 * 2^i + 1$ 和 $4^i - 3 * 2^i + 1$ 这两个算式。

#### 4.6 选择排序
**描述**
选择排序的原理是首先从数组中找到最小或者最大值，然后跟最开始或者最末尾的值交换，接着在除了最开始或者最末尾的数组内找最小或者最大值，然后跟最开始或者最末尾的值交换，以此类推。

**优化**
找到最小值的时候，同时找到最大值，然后将最小值和最大值各自放在对应的位置。

#### 4.7 计数排序
**描述**

#### 4.8 基数排序

#### 4.8 桶排序

先把这几个排序搞好
然后回过头梳理一下总体的使用场景以及分类，复习时间复杂度和空间复杂度的计算
然后接着看二叉树的定义
然后整理二叉树的相关算法

4.7 堆排序
4.8 拓扑排序


### 2、二叉树的遍历算法
#### 2.4.7、遍历

相比之下，遍历反而不是很复杂，一般从路径的角度有以下方式，从方向的角度，前三种属于深度优先，第四种属于广度优先
- 前序遍历：根结点->左结点->右结点
- 中序遍历：左结点->根结点->右结点
- 后序遍历：左结点->右结点>根结点-
- 层序遍历：一层一层

实现代码也写在了[AVL树具体实现代码](code/avl.py)

> 递归是很耗费计算机资源的，所以我们在写程序的时候要尽量避免使用递归。幸运的是，绝大部分递归的代码都有相应的迭代版本

这个我倒没有考虑过，说起来使用递归的场合也并没有遇到很多。二叉树遍历的递归实现很简单，但迭代方式却很绕。除了后序遍历之前，其他遍历的迭代方式已经实现，基本应该是没有问题的。<font color=red>后序遍历的迭代实现过于复杂</font>，参考文章一时也想不清楚，以后再说。

## 三、语言中的实现
- [ ] python的实现单独一个文件，其他语言的也是，还要写一下每个语言常用数据结构的区别，以及底层原理上的区别

python底层实现
> 在实际的使用当中，一般我们除非有需求或者报错，不然不会注意到底层的实现，但面试就会考这些，那么到底怎样才会在平凡使用语言的日常中注意到底层并且学习呢？只有好奇，只有多问一句它是如何实现的，然后才能了解到这些知识。

###  list和tuple
参考文章：
- python列表和元组：https://www.jianshu.com/p/24090fb63968
- python实现单链表，看看就行屁用没有：https://m.yisu.com/zixun/158827.html
- python list原理：https://zhuanlan.zhihu.com/p/143223943

```python
# 其实在python中没必要实现线性表，list类型足够用，但是即便如此，也应该了解list是如何实现的
# 单链表
class LNode():
    def __init__(self, value, next_=None):
        self.value = value
        self.next = next_
class LineLinkedList():
    def __init__(self):
        self.head = None
        self.num = 0
# 累了，不想写了。。，
# 双链表要加个唯一id识别出头节点
# 循环链表就再加个指向前面的属性
```
[别人实现的代码，作为参考](code/python_list.py)

问题：
- 1、python内部的数组是如何实现的？

```
# cpython中list的定义
typedef struct {
    PyObject_VAR_HEAD
    PyObject **ob_item;
    Py_ssize_t allocated;
} PyListObject;
```
首先python中的数组是`动态数组`，基本由cpython实现，PyObject是指向列表元素的指针列表，allocated是在内存中分配的插槽数，一般插槽数大于len，这样添加元素时，如果插槽数还剩余就不用申请空间；一旦超过，就需要申请新的空间，每次申请的插槽数不一样，以某种规律增长。
```
# resize
arguments: list object, new size
returns: 0 if OK, -1 if not
list_resize:
    new_allocated = (newsize >> 3) + (newsize < 9 ? 3 : 6) = 3
    new_allocated += newsize = 3 + 1 = 4
    resize ob_item (list of pointers) to size new_allocated
    return 0
# >>：移位运算符
# condition ? value1: value2: 条件判断府，如果符合就是value1，不然value2
```
其次复杂度分析，Appand 操作的时间复杂度 O(1)，Insert 操作的复杂度为O(n)，Pop 操作的复杂度为 O(1)，Remove操作的复杂度为O(n)，都挺好理解的，但是值得注意的是`当删除元素导致列表元素数量小于插槽数一半的时候，插槽数也会减少，即预设的内存会减少`

- 2、python可以实现内存申请吗，像c语言中的数组一样？

在python中tuple很像c语言的数组，创建后不可删除，修改；但是可以实现两个元组的合并，逻辑也很简单，新创建一个元组，读取已有元组中的元素复制到新元组中；元组相较于数组，除了保证数据安全外， 是更加内存友好的结构，即`占用内存更少、创建速度更快、与python普通回收不同，元组删除后，内存不会立刻返回给os暂留，可避免频繁与os交互`（这应该是优势吧？暂留真的不会影响效率吗？）

list各种操作的时间复杂度
```
index O(1)
pop O(1)
max/min O(k)
in O(n)
len O(1)
sort O(nlogn)
```

### dict

### set

# 其他

## 时间复杂度和空间复杂度
参考文章：[算法的时间与空间复杂度（一看就懂）](https://zhuanlan.zhihu.com/p/50479555)

这部分需要花时间专门攻克一下

## 递归思想
[手把手刷二叉树](https://labuladong.gitbook.io/algo/di-yi-zhang-shou-ba-shou-shua-shu-ju-jie-gou/shou-ba-shou-shua-er-cha-shu)

1、重复操作
2、重复变量
3、定义结果

我在整理知识点的时候并没有特别的感触，但在做题的时候，明知道可以用递归处理的问题，却总是无法流畅清晰地写出相映的代码；对于递归，是看似简单，但是需要整理一下如何分析和处理的思路。

在参考文章中理解到重复操作就像是二叉树遍历里输出root的值，也就是对当前结点的处理，而重复调用就相当于选择先传入左结点或右结点

```
def get_node(node):
	result = list()
	result.append(node.value)
	if node.left:
		result += get_node(node.left)
	if node.right:
		result  += get_node(node.right)
	return result
```



## 探究排序算法的稳定性

## 二叉树和图的唯一结点问题
如果处理值相同的结点，如何区分？